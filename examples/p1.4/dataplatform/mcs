[WARNING] MAPR_SERVER not set - falling back to default '/opt/mapr/server'
[WARNING] MAPR_TSDB_HOSTS not set - falling back to default '__TSDB_HOSTS_CHANGE_ME__'. Needs to be edited during startup
[WARNING] MAPR_ES_HOSTS not set - falling back to default '__ES_HOSTS_CHANGE_ME__'. Needs to be edited during startup
[WARNING] SECRETS_HOME not set - falling back to default '/opt/mapr/kubernetes'
[WARNING] SSH_PORT not set - falling back to default '22'
[WARNING] USE_WARDEN not set - falling back to default 'false'
[WARNING] MAPR_DB not set - falling back to default 'true'
[WARNING] REPLACE_DIR not set - falling back to default '/opt/mapr/kubernetes/replace-cm'
Not root user; Setting super user command sudo -E -n
Sudo SUDOCMD environment variable set to sudo -E -n
2021/03/09 02:55:54 common.sh: [INFO] === Start APISERVER ===
2021/03/09 02:55:54 common.sh: [INFO] Specifying container limits for MapR WEBSERVER components; Heap: 1024; Java: 1...
2021/03/09 02:55:54 common.sh: [INFO] Ensure container knows REAL memory limits from OS...
2021/03/09 02:55:54 common.sh: [INFO] REAL MEMORY FROM KUBERNETES = 2048 MB
2021/03/09 02:55:54 start.sh: [INFO] ...Success: Memory configuration completed
2021/03/09 02:55:54 start.sh: [INFO] Allocating max heap for component: WEBSERVER.
2021/03/09 02:55:54 start.sh: [INFO] Component is Java? True
2021/03/09 02:55:54 start.sh: [INFO] Min heap size: 1024
2021/03/09 02:55:54 start.sh: [INFO] Overhead memory: 204
2021/03/09 02:55:54 start.sh: [INFO] Pod available memory: 1844
2021/03/09 02:55:54 start.sh: [INFO] Min Memory needed: 1228
2021/03/09 02:55:54 start.sh: [INFO] Allocating memory...
2021/03/09 02:55:54 start.sh: [INFO] Allocating process heap for component: WEBSERVER.
2021/03/09 02:55:54 common.sh: [INFO] Service heap percent is: 85
2021/03/09 02:55:54 common.sh: [INFO] Name prefix is: WEBSERVER
2021/03/09 02:55:54 start.sh: [INFO] WEBSERVER_HEAPSIZE=1567
2021/03/09 02:55:54 start.sh: [INFO] Total memory required: 1771
2021/03/09 02:55:54 start.sh: [INFO] JAVA_MAXHEAP=32000
Logging set to: INFO
2021/03/09 02:55:54 common.sh: [INFO] Checking version...
2021/03/09 02:55:54 common.sh: [INFO] ...Success: Version check completed
2021/03/09 02:55:54 start.sh: [INFO] Configuring secrets...
2021/03/09 02:55:54 common.sh: [INFO] HSM configuration is only applicable to CLDB and ZK nodes. Not extracting to maprwebserver
2021/03/09 02:55:54 common.sh: [INFO] Copy MapR server ticket...
2021/03/09 02:55:54 common.sh: [INFO] MapR server ticket copied
2021/03/09 02:55:54 common.sh: [INFO] Copy other secrets...
2021/03/09 02:55:54 common.sh: [INFO] Server secrets processed
2021/03/09 02:55:54 common.sh: [INFO] Copy client secrets...
2021/03/09 02:55:54 common.sh: [INFO] Importing certs from the shared_ssl_truststore...
2021/03/09 02:55:56 common.sh: [INFO] Client secrets processed
2021/03/09 02:55:56 start.sh: [INFO] ...Success: Secrets copied
2021/03/09 02:55:56 start.sh: [INFO] Using SSSD...
2021/03/09 02:55:56 common.sh: [INFO] Updating LDAP conf...
cp: cannot stat '/opt/mapr/kubernetes/ldapcert-secrets/*': No such file or directory
2021/03/09 02:55:56 start.sh: [INFO] Updating SSSD conf...
2021/03/09 02:55:56 start.sh: [INFO] Starting SSSD...
2021/03/09 02:55:56 start.sh: [INFO] Setting conf dirs...
2021/03/09 02:55:56 common.sh: [INFO] Copying configuration files...
2021/03/09 02:55:56 common.sh: [INFO] ...Success: set_conf
2021/03/09 02:55:56 start.sh: [INFO] Setting up logging levels...
2021/03/09 02:55:56 start.sh: [INFO] The log4j properties file is already has the root logger set to INFO
2021/03/09 02:55:56 start.sh: [INFO] Setting up logging...
2021/03/09 02:55:56 start.sh: [INFO] Setting up core location...
2021/03/09 02:55:57 start.sh: [INFO] Setting up pod info...
2021/03/09 02:55:57 start.sh: [INFO] Setting hostname...
2021/03/09 02:55:57 common.sh: [INFO] Copying hostid files ...
2021/03/09 02:55:57 common.sh: [INFO] HostID contents: 5f48bfbd02adfc60
2021/03/09 02:55:57 start.sh: [INFO] ...Success: Hostname setup completed
2021/03/09 02:55:57 start.sh: [INFO] Configuring SSH...
2021/03/09 02:55:57 common.sh: [INFO] ...Success: SSH configuration completed
ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 
2021/03/09 02:55:58 start.sh: [INFO] Configuring env overrides...
2021/03/09 02:55:58 common.sh: [INFO] CLDB k8 hostname set to GKE-AGIRISH-CLUSTER-DEFAULT-POOL-F709ABD7-XRVV
2021/03/09 02:55:58 common.sh: [INFO] ...Success: CLDB k8 ip address set to 10.128.0.20,34.71.129.49
2021/03/09 02:55:58 common.sh: [INFO] adding ip 10.128.0.20,34.71.129.49 for the K8 host to env_override.sh
2021/03/09 02:55:58 common.sh: [INFO] ...Success: updated the env_override.sh file
2021/03/09 02:55:58 common.sh: [INFO] Running CONFIG.SH...
2021/03/09 02:55:58 common.sh: [INFO] MAPR_DB=true
2021/03/09 02:55:58 common.sh: [INFO] SECURE_CLUSTER=true
2021/03/09 02:55:58 common.sh: [INFO] Creating a secure cluster
externalzk=10.128.0.19:5181,10.128.0.18:5181,10.128.0.14:5181
2021/03/09 02:55:58 common.sh: [INFO] adding -EZ 10.128.0.19:5181,10.128.0.18:5181,10.128.0.14:5181...
2021/03/09 02:55:58 common.sh: [INFO] Calling sudo -E -n sudo -E -n /opt/mapr/server/configure.sh -no-autostart -on-prompt-cont y -v -f -nocerts -secure -u mapr -g mapr -N ag-cluster1 -C cldb-0.cldb-svc.ag-cluster1.svc.cluster.local, cldb-1.cldb-svc.ag-cluster1.svc.cluster.local, cldb-2.cldb-svc.ag-cluster1.svc.cluster.local -Z zk-0.zk-svc.ag-cluster1.svc.cluster.local, zk-1.zk-svc.ag-cluster1.svc.cluster.local, zk-2.zk-svc.ag-cluster1.svc.cluster.local -EZ 10.128.0.19:5181,10.128.0.18:5181,10.128.0.14:5181 -OT __TSDB_HOSTS_CHANGE_ME__ -ES __ES_HOSTS_CHANGE_ME__...
Using 7222 port for CLDB cldb-0.cldb-svc.ag-cluster1.svc.cluster.local
Using 7222 port for CLDB cldb-1.cldb-svc.ag-cluster1.svc.cluster.local
Using 7222 port for CLDB cldb-2.cldb-svc.ag-cluster1.svc.cluster.local
Using 5181 port for ZooKeeper zk-0.zk-svc.ag-cluster1.svc.cluster.local
Using 5181 port for ZooKeeper zk-1.zk-svc.ag-cluster1.svc.cluster.local
zk-1.zk-svc.ag-cluster1.svc.cluster.local: Unknown host
WARN: invalid(unresolvable) Zookeeper host/ip provided: zk-1.zk-svc.ag-cluster1.svc.cluster.local
Using 5181 port for ZooKeeper zk-2.zk-svc.ag-cluster1.svc.cluster.local
zk-2.zk-svc.ag-cluster1.svc.cluster.local: Unknown host
WARN: invalid(unresolvable) Zookeeper host/ip provided: zk-2.zk-svc.ag-cluster1.svc.cluster.local
Using 9200 port for Elasticsearch __ES_HOSTS_CHANGE_ME__
Using 4242 port for OpenTsdb __TSDB_HOSTS_CHANGE_ME__
CLDB node list: cldb-0.cldb-svc.ag-cluster1.svc.cluster.local:7222,cldb-1.cldb-svc.ag-cluster1.svc.cluster.local:7222,cldb-2.cldb-svc.ag-cluster1.svc.cluster.local:7222
Zookeeper node list: zk-0.zk-svc.ag-cluster1.svc.cluster.local:5181,zk-1.zk-svc.ag-cluster1.svc.cluster.local:5181,zk-2.zk-svc.ag-cluster1.svc.cluster.local:5181
External Zookeeper node list: 10.128.0.14:5181,10.128.0.18:5181,10.128.0.19:5181
Elasticsearch node list: __ES_HOSTS_CHANGE_ME__:9200
opentTsdb node list: __TSDB_HOSTS_CHANGE_ME__:4242

Node install STARTED
-----------------------
CMD: /opt/mapr/server/configure.sh -no-autostart -on-prompt-cont y -v -f -nocerts -secure -u mapr -g mapr -N ag-cluster1 -C cldb-0.cldb-svc.ag-cluster1.svc.cluster.local, cldb-1.cldb-svc.ag-cluster1.svc.cluster.local, cldb-2.cldb-svc.ag-cluster1.svc.cluster.local -Z zk-0.zk-svc.ag-cluster1.svc.cluster.local, zk-1.zk-svc.ag-cluster1.svc.cluster.local, zk-2.zk-svc.ag-cluster1.svc.cluster.local -EZ 10.128.0.19:5181,10.128.0.18:5181,10.128.0.14:5181 -OT __TSDB_HOSTS_CHANGE_ME__ -ES __ES_HOSTS_CHANGE_ME__
Cluster run as secure=true
Contructing ClusterConfFile: cldb node list: cldb-0.cldb-svc.ag-cluster1.svc.cluster.local:7222 cldb-1.cldb-svc.ag-cluster1.svc.cluster.local:7222 cldb-2.cldb-svc.ag-cluster1.svc.cluster.local:7222
Adding "ag-cluster1 secure=true cldb-0.cldb-svc.ag-cluster1.svc.cluster.local:7222 cldb-1.cldb-svc.ag-cluster1.svc.cluster.local:7222 cldb-2.cldb-svc.ag-cluster1.svc.cluster.local:7222" to "/opt/mapr/conf/mapr-clusters.conf"
Contructing ClusterConfFile: Done
Contructing MonitoringConfFile: openTsdb node list: __TSDB_HOSTS_CHANGE_ME__:4242
Contructing MonitoringConfFile: elasticsearch node list: __ES_HOSTS_CHANGE_ME__:9200
Contructing MonitoringConfFile: Done
MAPR_USER: mapr MAPR_GROUP: mapr
CREATE_USER:
maprUserId: maprGroupId:
Give privilleges to mapr
Config MAPR_USER for logs/conf of MapR Services
Update /opt/mapr/conf/daemon.conf
set mapr limits in /etc/security/limits.conf
mapr/mapr user/group configured
Node setup configuration:  apiserver hadoop-client hadoop-util hbase
Log can be found at:  /opt/mapr/logs/configure.log
Skipping ZooKeeper Role configuration... Not found
Skipping CLDB Role configuration... Not found
Skipping NFS Role configuration... Not found
Skipping NFS4 Role configuration... Not found
Updating Warden config
Adding "isDB=true" to "/opt/mapr/conf/warden.conf"
ECO_cmd: /opt/mapr/hadoop/hadoop-2.7.4/bin/configure.sh --secure -EC -OT __TSDB_HOSTS_CHANGE_ME__:4242 -ES __ES_HOSTS_CHANGE_ME__:9200 -nocerts -enableJMX true -jmxRemoteHost false -jmxLocalHost false -jmxLocalBinding true
Configuring hadoop-client
ECO_cmd: /opt/mapr/hbase/hbase-1.4.12/bin/configure.sh --secure -EC -OT __TSDB_HOSTS_CHANGE_ME__:4242 -ES __ES_HOSTS_CHANGE_ME__:9200 -nocerts -enableJMX true -jmxRemoteHost false -jmxLocalHost false -jmxLocalBinding true
Configuring hbase
ECO_cmd: /opt/mapr/apiserver/bin/configure.sh --secure -EC -OT __TSDB_HOSTS_CHANGE_ME__:4242 -ES __ES_HOSTS_CHANGE_ME__:9200 -nocerts -enableJMX true -jmxRemoteHost false -jmxLocalHost false -jmxLocalBinding true
Configuring apiserver
ECO_cmd: /opt/mapr/hadoop/hadoop-2.7.4/bin/hadoop_symlinks.sh --secure -EC -OT __TSDB_HOSTS_CHANGE_ME__:4242 -ES __ES_HOSTS_CHANGE_ME__:9200 -nocerts -enableJMX true -jmxRemoteHost false -jmxLocalHost false -jmxLocalBinding true
Configuring hadoop-util
Disksetup NOT run (-F or -D options not provided). Please run /opt/mapr/server/disksetup manually
Node not starting automatically.
Run "systemctl start mapr-warden" in order to start this node

Node install FINISHED
-----------------------
2021/03/09 02:56:05 common.sh: [INFO] ...Success: Configure.sh run completed
2021/03/09 02:56:05 start.sh: [INFO] Running statemonitor service...
2021/03/09 02:56:05 common.sh: [INFO] Waiting for CLDB to write status message...
2021/03/09 02:56:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:56:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:56:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:56:35 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:56:45 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:56:55 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:57:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:57:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:57:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:57:35 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:57:45 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:57:55 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:58:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:58:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:58:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:58:35 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:58:45 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:58:55 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:59:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:59:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:59:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:59:35 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:59:45 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 02:59:55 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:00:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:00:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:00:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:00:35 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:00:45 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:00:55 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:01:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:01:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:01:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:01:35 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:01:45 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:01:55 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:02:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:02:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:02:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:02:35 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:02:45 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:02:55 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:03:05 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:03:15 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:03:25 common.sh: [INFO] waiting for CLDB. Will retry in 10 seconds...
2021/03/09 03:03:35 common.sh: [INFO] Creating user ticket...
Successfully wrote the mapruserticket to /opt/mapr/conf/mapruserticket  
                                                                        
2021/03/09 03:03:38 common.sh: [INFO] Creating user ticket for mapr user...
MapR credentials of user 'mapr' for cluster 'ag-cluster1' are written to '/tmp/maprticket_5000'
2021/03/09 03:03:39 common.sh: [INFO] MAPR_TICKETFILE_LOCATION=/opt/mapr/conf/mapruserticket
Opening keyfile /opt/mapr/conf/mapruserticket
ag-cluster1: user = mapr, created = 'Tue Mar 09 03:03:38 UTC 2021', expires = 'Fri Jun 17 17:31:17 UTC 29229672', Not renewable, uid = 5000, gids = 5000, 5003, 0, CanImpersonate = true, isExternal = false
2021/03/09 03:03:40 common.sh: [INFO] ...Success: User ticket setup completed
2021/03/09 03:03:40 common.sh: [INFO] Starting Webserver service
apiserver (3436) started with log /opt/mapr/apiserver/logs/apiserver.log
2021/03/09 03:03:43 common.sh: [INFO] Running zkutils...
addServer called for  mcs-0.mcs-svc.ag-cluster1.svc.cluster.local
2021/03/09 03:03:43 Connected to 10.0.0.6:5181
2021/03/09 03:03:43 authenticated: id=144115273590702088, timeout=4000
2021/03/09 03:03:43 re-submitting `0` credentials after reconnect
2021/03/09 03:03:43 recv loop terminated: err=failed to read from connection: EOF
2021/03/09 03:03:43 send loop terminated: err=<nil>
panic: unknown error: -124

goroutine 1 [running]:
main.must(...)
	/go/src/zkutils/zkutils.go:37
main.addNode(0x6eface, 0x8)
	/go/src/zkutils/zkutils.go:86 +0x167
main.addServer(0xc00044c1e0, 0x2b)
	/go/src/zkutils/zkutils.go:155 +0x1cc
main.main()
	/go/src/zkutils/zkutils.go:283 +0xc9
2021/03/09 03:03:44 common.sh: [INFO] =============================================
2021/03/09 03:03:44 common.sh: [INFO] Useful Information:
2021/03/09 03:03:44 common.sh: [INFO] Contents of /conf/mapr-clusters.conf:
2021/03/09 03:03:44 common.sh: [INFO] ag-cluster1 secure=true cldb-0.cldb-svc.ag-cluster1.svc.cluster.local:7222 cldb-1.cldb-svc.ag-cluster1.svc.cluster.local:7222 cldb-2.cldb-svc.ag-cluster1.svc.cluster.local:7222
2021/03/09 03:03:44 common.sh: [INFO] Container host info:
2021/03/09 03:03:44 common.sh: [INFO] mcs-0 10.0.3.5 
2021/03/09 03:03:44 common.sh: [INFO] CPU Requested: 1
2021/03/09 03:03:44 common.sh: [INFO] CPU Limit: 2
2021/03/09 03:03:44 common.sh: [INFO] Memory Requested: 2048
2021/03/09 03:03:44 common.sh: [INFO] Memory Limit: 2048
2021/03/09 03:03:44 common.sh: [INFO] Preserve Container: false
2021/03/09 03:03:44 common.sh: [INFO] Pod Service Account: ag-cluster1
2021/03/09 03:03:44 common.sh: [INFO] K8S UID: 
2021/03/09 03:03:44 common.sh: [INFO] Pod Namespace: ag-cluster1
2021/03/09 03:03:44 common.sh: [INFO] Pod Name: mcs-0
2021/03/09 03:03:44 common.sh: [INFO] Pod Image Sha: 82420442aab093aa05645d831a76e78cd171a4c0
2021/03/09 03:03:44 common.sh: [INFO] Pod IP: 10.0.3.5
2021/03/09 03:03:44 common.sh: [INFO] Node Name: gke-agirish-cluster-default-pool-f709abd7-xrvv
2021/03/09 03:03:44 common.sh: [INFO] Host IP: 
2021/03/09 03:03:44 common.sh: [INFO] SSH Port: 22
2021/03/09 03:03:44 common.sh: [INFO] =============================================
2021/03/09 03:03:44 start.sh: [INFO] APISERVER pod is now really ready...
2021/03/09 03:03:44 start.sh: [INFO] === Sleeping forever to keep container up ===
